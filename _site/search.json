[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Building a semantic search tool for my Google Drive",
    "section": "",
    "text": "üìå TL;DR: I don‚Äôt really enjoy keyword-based search, which is what I believe Google Drive‚Äôs native search tool does. Here is my attempt at fixing it."
  },
  {
    "objectID": "index.html#introduction-why-im-interested-in-search",
    "href": "index.html#introduction-why-im-interested-in-search",
    "title": "Building a semantic search tool for my Google Drive",
    "section": "Introduction: Why I‚Äôm interested in search",
    "text": "Introduction: Why I‚Äôm interested in search\nFirst things first: Google Drive Search isn‚Äôt the worst search feature in the world. That award goes to Outlook Search, a feature that always enrages me. I don‚Äôt want to discredit the SWE‚Äôs and PM‚Äôs behind this tool; they are likely very smart, hard-working, and dedicated to shipping a high-quality feature; but every time I‚Äôm trying to find an old email, I find myself traversing the depths of my hippocampus to think of unusual keywords that the sender may have included. _Oh that‚Äôs right, I remembered they wrote ‚ÄòWith warm appreciation‚Äô, I‚Äôll search that up.‚Äù\nDocument search is an old problem with no obvious state-of-the-art solution. It‚Äôs conceptually hard, because it involves storing large pieces of text in small numerical representations, whether that be through count-vectorization or through neural embeddings, and it‚Äôs executionally hard, because a query‚Äôs runtime is often dependent on the size of the dataset. Consider the naive approach of iterating over every document in your Google Drive one-by-one to find the one closest to your query along some metric; if your Drive is 14.99 GB in size like mine is, that‚Äôs not gonna work so well.\nI decided to work on search more because I think it‚Äôs an interesting and open-ended problem, in the direction of what I‚Äôm more interested in, which is creating a really good AI-Native Productivity tool. I‚Äôm somewhat familiar with Google API and apps from previous projects, and I think that building custom Google Drive document search has a higher upside than an email filter does."
  },
  {
    "objectID": "index.html#the-end-product",
    "href": "index.html#the-end-product",
    "title": "Building a semantic search tool for my Google Drive",
    "section": "The end product",
    "text": "The end product\nThere are two core components to this search tool, a good representation function for my data and an algorithm for finding the document whose representation is closest to some query. Here I assume that the query and the document will be embed with the same function and the optimal search result will minimize the distance between their embeddings. Given that the query will represent much less information than the documents will typically contain, it might make sense to have a larger embedding for each of the documents. For example, having a dimension of the embedding vary with sequence length would allow for this.\nI don‚Äôt want my search tool to download and embed my entire Google Drive each time it is given a search query. Data representation should happen when I authenticate the app the first time, and updates should happen asynchronously. When I enter a search, the only thing that should need to happen is query embedding and comparison.\nIt made most sense to create a Google Drive app, and this app was not light in credentials. It requires access to see, edit, create, and delete documents in a user‚Äôs Google Drive, as well as see document metadata and personal user information (email address, just so a unique identifier exists for their credentials to be stored on my end). Google requires several days to verify apps like these, and then pretty much begs end users to use them anyways, which is probably a good privacy practice."
  },
  {
    "objectID": "index.html#implementation-similarity-search-on-sentence-embeddings",
    "href": "index.html#implementation-similarity-search-on-sentence-embeddings",
    "title": "Building a semantic search tool for my Google Drive",
    "section": "Implementation: Similarity Search on Sentence Embeddings",
    "text": "Implementation: Similarity Search on Sentence Embeddings\nIn my last post, I used FAISS as a method of efficiently searching for similar features by matching decoder weights. Since it was fresh in my mind, I thought why not start out with it. Since we‚Äôre working with documents here, I decided to use sentence transformers instead of regular transformers; sentence transformers are designed to perform well at semantic search, clustering, and information retrieval."
  },
  {
    "objectID": "index.html#evals-and-hypothetical-benchmarks",
    "href": "index.html#evals-and-hypothetical-benchmarks",
    "title": "Building a semantic search tool for my Google Drive",
    "section": "Evals and hypothetical benchmarks",
    "text": "Evals and hypothetical benchmarks\n\nSynthetic Dataset Generation\nWe need to obtain a document corpus and a set of queries to pass into that document corpus. A good first pass solution is to make a synthetic benchmark, because we can control ground truth instead of having to compute it. The heavy-lifting in any of these approaches will be done with an LLM.\nIn a first-pass approach, we can generate borrow intuition from the ancient topic modeling literature. Here, we‚Äôll generate a list of non-overlapping topics or themes, with some degree of specifity (e.g.¬†‚ÄúReviews of classical literature from the 20th century‚Äù, ‚ÄúHistory of the two-party system‚Äù, ‚ÄúApartment hunting in Chicago.‚Äù I prompted ChatGPT to come up with some more, and it generated: Travel Planning and Reviews, Educational Essays and Notes, Personal Finance and Budgeting, Creative Writing and Fiction‚Ä¶ Definitely want some more specific topics than these, but this was more a result of my lazy prompting.\nFrom there, we prompt an LLM to generate 3-5 documents of length 100-500 words for each topic, and build our corpus as such. After this, we also ask the LLM to generate 3-5 queries of length 5-20 words for each topic.\n\n\nReal datasets (and the ground truth problem)\nIt seems necessary that any high-quality benchmark have a real data component in addition to the synthetic data component. If we are going with real data, how are we going to get ground truth? Assuming the corpus and query set already exists, a benchmark can be created by employing some extremely time-consuming but high-fidelity search approach; it‚Äôs not hard to think of high-quality search tactics that are impractical because of their slow runtime. These approaches can be applied to benchmark creation.\nHere‚Äôs an example approach for a real document and set of queries: for each query, iterate over the entire corpus and ask an LLM whether the document is relevant to that query. Once we‚Äôve filtered down the corpus to only the documents that are relevant to the query, we can pass the entire sub-corpus into an LLM and ask it to find the documents that are most relevant. This can be considered a ground-truth set of search results."
  }
]