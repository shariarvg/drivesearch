---
title: "Building a semantic search tool for my Google Drive"
author: "Shariar Vaez-Ghaemi"
date: "2025-07-01"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    code-tools: true
    include-in-header: ref_to_custom_css.html
---

::: {.tldr}
ðŸ“Œ **TL;DR**: I don't really enjoy keyword-based search, which is what I believe Google Drive's native search tool does. Here is my attempt at fixing it.
:::

![How drivesearch appears](screenshot_docs_search)

## Introduction: Why I'm interested in search

First things first: Google Drive Search isn't the _worst_ search feature in the world. That award goes to Outlook Search, a feature that always enrages me. I don't want to discredit the SWE's and PM's behind this tool; they are likely very smart, hard-working, and dedicated to shipping a high-quality feature; but every time I'm trying to find an old email, I find myself traversing the depths of my hippocampus to think of unusual keywords that the sender may have included. _Oh that's right, I remembered they wrote 'With warm appreciation', I'll search that up."

Document search is an old problem with no obvious state-of-the-art solution. It's conceptually hard, because it involves storing large pieces of text in small numerical representations, whether that be through count-vectorization or through neural embeddings, and it's executionally hard, because a query's runtime is often dependent on the size of the dataset. Consider the naive approach of iterating over every document in your Google Drive one-by-one to find the one closest to your query along some metric; if your Drive is 14.99 GB in size like mine is, that's not gonna work so well.

I decided to work on search more because I think it's an interesting and open-ended problem, in the direction of what I'm more interested in, which is creating a really good AI-Native Productivity tool. I'm somewhat familiar with Google API and apps from previous projects, and I think that building custom Google Drive document search has a higher upside than an email filter does. 

## The end product

There are two core components to this search tool, a good representation function for my data and an algorithm for finding the document whose representation is closest to some query. Here I assume that the query and the document will be embed with the same function and the optimal search result will minimize the distance between their embeddings. Given that the query will represent much less information than the documents will typically contain, it might make sense to have a larger embedding for each of the documents. For example, having a dimension of the embedding vary with sequence length would allow for this.

I don't want my search tool to download and embed my entire Google Drive each time it is given a search query. Data representation should happen when I authenticate the app the first time, and updates should happen asynchronously. When I enter a search, the only thing that should need to happen is query embedding and comparison.

It made most sense to create a Google Drive app, and this app was not light in credentials. It requires access to see, edit, create, and delete documents in a user's Google Drive, as well as see document metadata and personal user information (email address, just so a unique identifier exists for their credentials to be stored on my end). Google requires several days to verify apps like these, and then pretty much begs end users to use them anyways, which is probably a good privacy practice.

## Implementation: Similarity Search on Sentence Embeddings

In my [last post](https://shariarvg.github.io/recurring_features/), I used [FAISS](https://github.com/facebookresearch/faiss) as a method of efficiently searching for similar features by matching decoder weights. Since it was fresh in my mind, I thought why not start out with it. Since we're working with documents here, I decided to use [sentence transformers](https://sbert.net/) instead of regular transformers; sentence transformers are designed to perform well at semantic search, clustering, and information retrieval. 

## Evals and hypothetical benchmarks

### Synthetic Dataset Generation

We need to obtain a document corpus and a set of queries to pass into that document corpus. A good first pass solution is to make a synthetic benchmark, because we can control ground truth instead of having to compute it. The heavy-lifting in any of these approaches will be done with an LLM.

In a first-pass approach, we can generate borrow intuition from the ancient topic modeling literature. Here, we'll generate a list of non-overlapping topics or themes, with some degree of specifity (e.g. "Reviews of classical literature from the 20th century", "History of the two-party system", "Apartment hunting in Chicago." I prompted ChatGPT to come up with some more, and it generated: Travel Planning and Reviews, Educational Essays and Notes, Personal Finance and Budgeting, Creative Writing and Fiction... Definitely want some more specific topics than these, but this was more a result of my lazy prompting.

From there, we prompt an LLM to generate 3-5 documents of length 100-500 words for each topic, and build our corpus as such. After this, we also ask the LLM to generate 3-5 queries of length 5-20 words for each topic. 


### Real datasets (and the ground truth problem)

It seems necessary that any high-quality benchmark have a real data component in addition to the synthetic data component. If we are going with real data, how are we going to get ground truth? Assuming the corpus and query set already exists, a benchmark can be created by employing some extremely time-consuming but high-fidelity search approach; it's not hard to think of high-quality search tactics that are impractical because of their slow runtime. These approaches can be applied to benchmark creation.

Here's an example approach for a real document and set of queries: for each query, iterate over the entire corpus and ask an LLM whether the document is relevant to that query. Once we've filtered down the corpus to only the documents that are relevant to the query, we can pass the entire sub-corpus into an LLM and ask it to find the documents that are most relevant. This can be considered a ground-truth set of search results.


